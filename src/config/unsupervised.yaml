hydra:
  run:
    dir: ${oc.env:SCRATCH,.}/outputs/unsupervised/${now:%Y-%m-%d}/${now:%H-%M-%S}
group: supervised-latent
data:
  dataset_name: wmt14
  language_pair: de-en
  tokenizer_path_a: deepset/gbert-base
  tokenizer_path_b: bert-base-cased
  stream: False
  train_split: train
  val_split: validation
  num_workers: 8
model:
  use_oracle: True
  pooling: attention
  n_pools: 8
  latent_regularizer: vq
  use_latent_projection: false
  distance_metric: l2
  num_encoder_layers: 4
  num_decoder_layers: 6
  vq:
    n_codes: 2048
    n_groups: 8
training:
  batch_size: 64
  devices: 4
  accumulate_batches: 1
  beta_cycle: 0.1
  beta_vq: 0.1
  beta_cycle_warmup_steps: 0
  beta_vq_warmup_steps: 100
  cycle:
    num_beams: 1
    do_sample: true
  optimizer:
    lr: 1e-4
    warmup_steps: 0
    max_steps: 100000
    schedule: cosine
  max_steps: 100000
  max_epochs: 50
  max_seq_len: 128
  strategy: dp
  val:
    check_interval: 256
    limit_batches: 128
