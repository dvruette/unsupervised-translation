hydra:
  run:
    dir: ${oc.env:SCRATCH,.}/outputs/unsupervised/${now:%Y-%m-%d}/${now:%H-%M-%S}
group: supervised-latent
data:
  dataset_name: wmt14
  language_pair: de-en
  tokenizer_path_a: deepset/gbert-base
  tokenizer_path_b: bert-base-cased
  stream: False
  train_split: train
  val_split: validation
  num_workers: 8
model:
  use_oracle: True
  pooling: attention
  n_pools: 8
  latent_regularizer: critic
  use_latent_projection: false
  num_encoder_layers: 4
  num_decoder_layers: 6
training:
  batch_size: 64
  devices: 4
  accumulate_batches: 1
  beta_critic: 1.0
  critic_loss: classifier
  optimizer:
    lr_rec: 1e-4
    lr_critic: 2e-5
    lr_enc: 2e-5
    n_critic_steps: 5
    warmup_steps: 0
    max_steps: 100000
    schedule: cosine
  max_steps: 100000
  max_epochs: 50
  max_seq_len: 128
  strategy: dp
  val:
    check_interval: 256
    limit_batches: 128
