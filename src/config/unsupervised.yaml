hydra:
  run:
    dir: ${oc.env:SCRATCH,.}/outputs/unsupervised/${now:%Y-%m-%d}/${now:%H-%M-%S}
data:
  dataset_name: wmt14
  language_pair: de-en
  tokenizer_path_a: deepset/gbert-base
  tokenizer_path_b: bert-base-cased
  stream: False
  train_split: train
  val_split: validation
  num_workers: 8
model:
  use_oracle: True
  pooling: attention
  n_pools: 4
  latent_regularizer: vq
  distance_metric: l2
  num_encoder_layers: 4
  num_decoder_layers: 6
  vq:
    n_codes: 512
    n_groups: 16
training:
  batch_size: 16
  accumulate_batches: 1
  beta_cycle: 0.25
  beta_vq: 0.25
  beta_cycle_warmup_steps: 4000
  beta_vq_warmup_steps: 0
  optimizer:
    lr: 1e-4
    warmup_steps: 0
    max_steps: 100000
    schedule: cosine
  max_steps: 100000
  max_epochs: 50
  max_seq_len: 128
  strategy: dp
  val:
    check_interval: 256
    limit_batches: 128
